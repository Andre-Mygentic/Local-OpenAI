# ğŸ¤– OpenAI GPT-OSS 20B - Run Locally on Mac

Run OpenAI's powerful GPT-OSS 20B model locally on your Mac with a beautiful Streamlit UI. One-click installation, no cloud required, 100% private.

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Platform](https://img.shields.io/badge/platform-macOS-lightgrey.svg)
![Python](https://img.shields.io/badge/python-3.9+-green.svg)

## âœ¨ Features

- ğŸš€ **One-Command Installation** - Get up and running in minutes
- ğŸ’» **Beautiful Web UI** - Clean Streamlit interface for chatting with GPT-OSS
- ğŸ”’ **100% Local & Private** - Your data never leaves your machine
- âš¡ **Optimized for Apple Silicon** - Leverages Metal Performance Shaders
- ğŸ“Š **Real-time Stats** - Monitor performance, response times, and memory usage
- ğŸ’¾ **Chat History** - Export and save your conversations

## ğŸ“‹ Requirements

- **macOS** (Apple Silicon M1/M2/M3/M4 or Intel)
- **16GB+ RAM** (36GB recommended for best performance)
- **14GB free disk space** for the model
- **Python 3.9+** (usually pre-installed on Mac)

## ğŸš€ Quick Start

### Step 1: Clone the Repository
```bash
git clone https://github.com/yourusername/ollama-gpt-oss.git
cd ollama-gpt-oss
```

### Step 2: Run the Setup Script (One-Time)
```bash
chmod +x setup.sh
./setup.sh
```

This script will automatically:
- âœ… Install Homebrew (if needed)
- âœ… Install Ollama
- âœ… Download the GPT-OSS 20B model (13GB)
- âœ… Create Python virtual environment
- âœ… Install all dependencies
- âœ… Verify everything is working

**Note:** The model download may take 10-30 minutes depending on your internet speed.

### Step 3: Start the UI
```bash
./run_ollama_ui.sh
```

Then open your browser to: **http://localhost:8501**

That's it! You're now running GPT-OSS 20B locally! ğŸ‰

## ğŸ’¬ Usage

### Web UI (Recommended)
After running `./run_ollama_ui.sh`, you can:
- Chat with the model in real-time
- Adjust temperature and max tokens
- Export chat history
- Monitor performance metrics

### Command Line
```bash
ollama run gpt-oss:20b
```

### Python API
```python
import requests

response = requests.post('http://localhost:11434/api/generate', 
    json={
        "model": "gpt-oss:20b",
        "prompt": "Hello, how are you?"
    })
print(response.json()['response'])
```

## ğŸ“ Project Structure
```
ollama-gpt-oss/
â”œâ”€â”€ setup.sh                   # One-time setup script
â”œâ”€â”€ run_ollama_ui.sh          # Start the Streamlit UI
â”œâ”€â”€ ollama_streamlit_app.py   # Streamlit application
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ ollama_api_example.py     # API usage examples
â””â”€â”€ README.md                 # This file
```

## ğŸ› ï¸ Troubleshooting

### Model Download Issues
If the download is interrupted, simply run:
```bash
ollama pull gpt-oss:20b
```
Ollama will resume from where it left off.

### Port Already in Use
If port 8501 is busy, you can specify a different port:
```bash
streamlit run ollama_streamlit_app.py --server.port 8502
```

### Memory Issues
GPT-OSS 20B requires significant RAM. If you experience issues:
- Close other applications
- Consider using a smaller model: `ollama pull llama3.1:8b`

## ğŸ”§ Manual Installation

If you prefer to install components manually:

1. **Install Ollama:**
```bash
brew install ollama
brew services start ollama
```

2. **Download the model:**
```bash
ollama pull gpt-oss:20b
```

3. **Setup Python environment:**
```bash
python3 -m venv ollama_ui_env
source ollama_ui_env/bin/activate
pip install -r requirements.txt
```

4. **Run the UI:**
```bash
streamlit run ollama_streamlit_app.py
```

## ğŸ¯ Performance

On Apple M4 Max (36GB RAM):
- **Model Load Time:** ~5 seconds
- **Average Response Time:** 2-3 seconds
- **Memory Usage:** 8-10GB when active
- **Token Generation:** ~30-50 tokens/second

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- OpenAI for releasing GPT-OSS as open-source
- Ollama team for the excellent local LLM runtime
- Streamlit for the amazing web framework

## â­ Star History

If you find this useful, please consider giving it a star!

## ğŸ“® Contact

For issues, questions, or suggestions, please open an issue on GitHub.

---

**Made with â¤ï¸ for the open-source community**